{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 12: Deep Learning in Multi-Player Games\n",
    "\n",
    "New Skills in This Chapter:\n",
    "\n",
    "• Creating your own game environment\n",
    "\n",
    "• Adding attributes and methods to a game environment\n",
    "\n",
    "• Making moves and determining wins and losses in a game\n",
    "\n",
    "• Training game strategies in self-made environments\n",
    "\n",
    "• Designing deep learning game strategies in multi-player games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "*Oh, well, this would be one of those circumstances that people unfamiliar with the\n",
    "law of large numbers would call a coincidence.*<br>\n",
    "***\n",
    "--Sheldon Cooper, in The Big Bang Theory\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch12\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5288bb7",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Install needed modules for Chapter 12}}$<br>\n",
    "***\n",
    "To covert a ps file to a png file, you need to conda install Ghostscript.  \n",
    "\n",
    "Enter the following command in the Anaconda prompt (Windows) or a terminal (MAC/Linux) with your virtual environment activated \n",
    "\n",
    "\n",
    "`conda install -c conda-forge ghostscript==9.54.0`\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 12.1. Create the Tic Tac Toe Game Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d7767",
   "metadata": {},
   "source": [
    "## 12.1.2. Create A Local Module for the Tic Tac Toe Game\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252a608",
   "metadata": {},
   "source": [
    "Download *ttt_env.py* in the folder *utils* from the book's GitHub repository. \n",
    "\n",
    "Open the file *ttt_env.py* to familiarize yourself with the module. To save space, we'll just outline the main structure of the module below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d59609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an action_space helper class\n",
    "class action_space:\n",
    "    ...\n",
    "    \n",
    "# Define an obervation_space helper class    \n",
    "class observation_space:\n",
    "    ...\n",
    "\n",
    "# the ttt class\n",
    "class ttt():\n",
    "    # initiate the class\n",
    "    def __init__(self): \n",
    "        ...\n",
    "    # reset the board\n",
    "    def reset(self):  \n",
    "        ...            \n",
    "    # place piece on board and update state\n",
    "    def step(self, inp):\n",
    "        ...\n",
    "                  \n",
    "    # Determine if a player has won the game\n",
    "    def win_game(self):\n",
    "        ...\n",
    "    # Show the graphical board\n",
    "    def render(self):\n",
    "        ...\n",
    "    # Close the game environment\n",
    "    def close(self):\n",
    "        ...        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "## 12.1.3. Verify the Custom-Made Game Environment\n",
    "First we'll initiate the game environment and show the game board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a847d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ttt_env import ttt\n",
    "\n",
    "env = ttt()\n",
    "env.reset()                    \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e6bd0a",
   "metadata": {},
   "source": [
    "We first import the *ttt* class from the local package. We then create an instance of the class and call it *env*. The *reset()* method set the game board to the initial state. The *render()* method generates a graphical game board. \n",
    "\n",
    "If you run the above cell, you should see a separate turtle window, with a game board as follows: \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/ttt_start.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0977029b",
   "metadata": {},
   "source": [
    "If you want to close the game board window, use the *close()* method, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439f093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963a4e3",
   "metadata": {},
   "source": [
    "Next, we'll check the attributes of the game environment such as the observation space and the action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of possible actions are 9\n",
      "the following are ten sample actions\n",
      "2\n",
      "5\n",
      "1\n",
      "8\n",
      "8\n",
      "1\n",
      "9\n",
      "4\n",
      "2\n",
      "3\n",
      "the shape of the observation space is (9,)\n"
     ]
    }
   ],
   "source": [
    "# check the action space\n",
    "number_actions = env.action_space.n\n",
    "print(\"the number of possible actions are\",\\\n",
    "      number_actions)\n",
    "# sample the action space ten times\n",
    "print(\"the following are ten sample actions\")\n",
    "for i in range(10):\n",
    "   print(env.action_space.sample())\n",
    "# check the shape of the observation space\n",
    "print(\"the shape of the observation space is\", \\\n",
    "      env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033eed1",
   "metadata": {},
   "source": [
    "Results above show that there are nine possible actions that can be taken by the agent. The meanings of the actions in this game as follows\n",
    "* 1: Placing a game piece in cell 1\n",
    "* 2: Placing a game piece in cell 2\n",
    "* ...\n",
    "* 9: Placing a game piece in cell 9\n",
    "\n",
    "The *sample()* method returns an action from the action space randomly. The state space is a vector with 9 values. Each value can be either -1, 0, or 1, with teh following meanings: \n",
    "* 0 means the cell is empty; \n",
    "* -1 means the cell is occupied by player O; \n",
    "* 1 means the cell is occupied by player X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f879a",
   "metadata": {},
   "source": [
    "## 12.1.4. Play A Game in the Tic Tac Toe Environment\n",
    "Next, we'll play a game in the custom-made environment, by randomly choosing an action from the action space each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44d0601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player X has chosen action 8\n",
      "the current state is \n",
      "[[0 1 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Player O has chosen action 9\n",
      "the current state is \n",
      "[[ 0  1 -1]\n",
      " [ 0  0  0]\n",
      " [ 0  0  0]]\n",
      "Player X has chosen action 4\n",
      "the current state is \n",
      "[[ 0  1 -1]\n",
      " [ 1  0  0]\n",
      " [ 0  0  0]]\n",
      "Player O has chosen action 2\n",
      "the current state is \n",
      "[[ 0  1 -1]\n",
      " [ 1  0  0]\n",
      " [ 0 -1  0]]\n",
      "Player X has chosen action 1\n",
      "the current state is \n",
      "[[ 0  1 -1]\n",
      " [ 1  0  0]\n",
      " [ 1 -1  0]]\n",
      "Player O has chosen action 5\n",
      "the current state is \n",
      "[[ 0  1 -1]\n",
      " [ 1 -1  0]\n",
      " [ 1 -1  0]]\n",
      "Player X has chosen action 3\n",
      "the current state is \n",
      "[[ 0  1 -1]\n",
      " [ 1 -1  0]\n",
      " [ 1 -1  1]]\n",
      "Player O has chosen action 6\n",
      "the current state is \n",
      "[[ 0  1 -1]\n",
      " [ 1 -1 -1]\n",
      " [ 1 -1  1]]\n",
      "Player X has chosen action 7\n",
      "the current state is \n",
      "[[ 1  1 -1]\n",
      " [ 1 -1 -1]\n",
      " [ 1 -1  1]]\n",
      "Player X has won!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "env=ttt()\n",
    "state=env.reset()   \n",
    "env.render()\n",
    "# Play a full game\n",
    "while True:\n",
    "    action = random.choice(env.validinputs)\n",
    "    time.sleep(1)\n",
    "    print(f\"Player X has chosen action {action}\")    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)[::-1]}\")    \n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"Player X has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break   \n",
    "    action = random.choice(env.validinputs)\n",
    "    time.sleep(1)\n",
    "    print(f\"Player O has chosen action {action}\")    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)[::-1]}\") \n",
    "    if done:\n",
    "        print(f\"Player O has won!\") \n",
    "        break   \n",
    "env.close()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbdf00",
   "metadata": {},
   "source": [
    "# 12.2. Train A Deep Learning Game Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c5b19",
   "metadata": {},
   "source": [
    "## 12.2.2. Simulate Tic Tac Toe Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc2f8827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 0, 0, 0, 1, 0, 0, 0, 0]),\n",
      " array([ 0,  0,  0,  0,  1,  0, -1,  0,  0]),\n",
      " array([ 0,  0,  0,  0,  1,  0, -1,  1,  0]),\n",
      " array([ 0,  0, -1,  0,  1,  0, -1,  1,  0]),\n",
      " array([ 0,  1, -1,  0,  1,  0, -1,  1,  0])]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "env=ttt()\n",
    "# Define the one_game() function\n",
    "def one_game():\n",
    "    history=[]\n",
    "    state=env.reset()   \n",
    "    while True:   \n",
    "        action=random.choice(env.validinputs)  \n",
    "        state,reward,done,info=env.step(action)\n",
    "        history.append(np.array(state))\n",
    "        if done:\n",
    "            break\n",
    "    return history, reward\n",
    "\n",
    "# Simulate one game and print out results\n",
    "history, outcome = one_game()\n",
    "pprint(history)\n",
    "pprint(outcome)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a6a600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate 100000 games and record them\n",
    "results = []        \n",
    "for x in range(100000):\n",
    "    history, outcome = one_game()\n",
    "    # Note here we associate each board with the game outcome\n",
    "    for board in history:\n",
    "        results.append((outcome, board))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88782a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, array([0, 0, 0, 1, 0, 0, 0, 0, 0])),\n",
      " (1, array([ 0,  0,  0,  1,  0,  0,  0, -1,  0])),\n",
      " (1, array([ 0,  0,  0,  1,  0,  0,  0, -1,  1])),\n",
      " (1, array([ 0,  0,  0,  1,  0, -1,  0, -1,  1])),\n",
      " (1, array([ 0,  1,  0,  1,  0, -1,  0, -1,  1])),\n",
      " (1, array([ 0,  1, -1,  1,  0, -1,  0, -1,  1])),\n",
      " (1, array([ 1,  1, -1,  1,  0, -1,  0, -1,  1])),\n",
      " (1, array([ 1,  1, -1,  1,  0, -1, -1, -1,  1])),\n",
      " (1, array([ 1,  1, -1,  1,  1, -1, -1, -1,  1])),\n",
      " (0, array([0, 0, 0, 0, 1, 0, 0, 0, 0]))]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# save the simulation data on your computer\n",
    "with open('files/ch12/games_ttt100K.p', 'wb') as fp:\n",
    "    pickle.dump(results,fp)\n",
    "# read the data and print out the first 10 observations       \n",
    "with open('files/ch12/games_ttt100K.p', 'rb') as fp:\n",
    "    games = pickle.load(fp)\n",
    "pprint(games[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e27a43",
   "metadata": {},
   "source": [
    "## 12.2.3. Train Your Tic Tac Toe Game Strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1579eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=128, \n",
    "kernel_size=(3,3),padding=\"same\",activation=\"relu\",\n",
    "                 input_shape=(3,3,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='adam', \n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67ebf802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with open('files/ch12/games_ttt100K.p', 'rb') as fp:\n",
    "    tttgames = pickle.load(fp)\n",
    "\n",
    "boards = []\n",
    "outcomes = []\n",
    "for game in tttgames:\n",
    "    boards.append(game[1])\n",
    "    outcomes.append(game[0])\n",
    "\n",
    "X = np.array(boards).reshape((-1, 3, 3, 1))\n",
    "# one_hot encoder, three outcomes: -1, 0, and 1\n",
    "y = tf.keras.utils.to_categorical(outcomes, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0faa6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 100 epochs\n",
    "model.fit(X, y, epochs=100, verbose=0)\n",
    "model.save('files/ch12/trained_ttt100K.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d20d76",
   "metadata": {},
   "source": [
    "It takes several hours to train the model since we have close to a million observations. The trained model is saved on your computer. Alternatively, you can download the trained model from the book’s GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14805aac",
   "metadata": {},
   "source": [
    "# 12.3. Use the Trained Model to Play Games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c57b7d8",
   "metadata": {},
   "source": [
    "## 12.3.1. Best Moves Based on the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96c8710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def best_move_X(env):\n",
    "    # if there is only one valid move, take it\n",
    "    if len(env.validinputs)==1:\n",
    "        return env.validinputs[0]\n",
    "    # Set the initial value of bestoutcome        \n",
    "    bestoutcome=-2;\n",
    "    bestmove=None    \n",
    "    #go through all possible moves hypothetically \n",
    "    for move in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        state=state.reshape(-1,3,3,1)\n",
    "        prediction=model.predict(state, verbose=0)\n",
    "        # output is prob(X wins) - prob(O wins)\n",
    "        win_lose_dif=prediction[0][1]-prediction[0][2]\n",
    "        if win_lose_dif>bestoutcome:\n",
    "            # Update the bestoutcome\n",
    "            bestoutcome=win_lose_dif\n",
    "            # Update the best move\n",
    "            bestmove=move\n",
    "    return bestmove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d262179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_move_O(env):\n",
    "    # Set the initial value of bestoutcome        \n",
    "    bestoutcome = -2;\n",
    "    bestmove=None    \n",
    "    #go through all possible moves hypothetically \n",
    "    for move in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        state=state.reshape(-1,3,3,1)\n",
    "        prediction=model.predict(state, verbose=0)\n",
    "        # output is prob(O wins) - prob(X wins)\n",
    "        win_lose_dif=prediction[0][2]-prediction[0][1]\n",
    "        if win_lose_dif>bestoutcome:\n",
    "            # Update the bestoutcome\n",
    "            bestoutcome = win_lose_dif\n",
    "            # Update the best move\n",
    "            bestmove = move\n",
    "    return bestmove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931fe8eb",
   "metadata": {},
   "source": [
    "## 12.3.2. Test A Game Against the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21f51838",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=ttt()\n",
    "state=env.reset()   \n",
    "env.render()\n",
    "# Play a full game manually\n",
    "while True:\n",
    "    # Use the best_move_X() function to select move\n",
    "    action=best_move_X(env)\n",
    "    print(f\"Player X has chosen action {action}\")    \n",
    "    state,reward,done,info=env.step(action)\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)[::-1]}\")\n",
    "    env.render()\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"Player X has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break    \n",
    "    action = random.choice(env.validinputs)\n",
    "    print(f\"Player O has chosen action {action}\")    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)[::-1]}\")\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(f\"Player O has won!\") \n",
    "        break        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f402a645",
   "metadata": {},
   "source": [
    "The best strategy looks at each possible next move, and add that move to the current board to form a hypothetical board. We feed the hypothetical board to the trained model to make predictions. \n",
    "The prediction will have three values: the probability of tying, player X winning, and player O winning. The best strategy chooses the move with the highest probability of player X winning the game. \n",
    "\n",
    "Here is one example of the eventual outcome:\n",
    "\n",
    "\n",
    "Player X uses the best moves recommended by the trained model and wins the game by occupying cells 4, 5, and 6, as shown in this picture.\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/ttt_win_screen.png\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40a6ec",
   "metadata": {},
   "source": [
    "You can also test the best strategy for player O by using the best_move_O() function, assuming Player X chooses random moves. I leave that as an excercise for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ece520",
   "metadata": {},
   "source": [
    "## 12.3.3. Test the Efficacy of the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98ed15f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "results=[]\n",
    "for i in range(1000):\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=random.choice(env.validinputs)\n",
    "        state, reward, done, info = env.step(action)\n",
    "    while True:\n",
    "        if env.turn==\"X\":\n",
    "            action = best_move_X(env) \n",
    "        else:\n",
    "            action = best_move_O(env)    \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the deep learning agent wins\n",
    "            if reward!=0:\n",
    "                results.append(1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break  \n",
    "        action = random.choice(env.validinputs)   \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is -1 if the deep learning agent loses\n",
    "            if reward!=0:\n",
    "                results.append(-1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d392cd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of winning games is 994\n",
      "the number of tying games is 6\n",
      "the number of losing games is 0\n"
     ]
    }
   ],
   "source": [
    "# count how many times the deep learning agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the deep learning agent has won {wins} games\")\n",
    "# count how many times the deep learning agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the deep learning agent has lost {losses} games\")         \n",
    "# count how many times the game ties\n",
    "losses=results.count(0)\n",
    "print(f\"the game has tied {losses} times\")          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3674b",
   "metadata": {},
   "source": [
    "# 12.4. Animate the Deep Learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2dd43",
   "metadata": {},
   "source": [
    "## 12.4.1. Probabilities of Winning for Each Hypothetical Move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f82fb767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch12util import record_ttt\n",
    "\n",
    "history=record_ttt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "431d5f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If Player X chooses action 1, the probability of winning is 0.6460.\n",
      "If Player X chooses action 2, the probability of winning is 0.5623.\n",
      "If Player X chooses action 3, the probability of winning is 0.6474.\n",
      "If Player X chooses action 4, the probability of winning is 0.5680.\n",
      "If Player X chooses action 5, the probability of winning is 0.7345.\n",
      "If Player X chooses action 6, the probability of winning is 0.5654.\n",
      "If Player X chooses action 7, the probability of winning is 0.6453.\n",
      "If Player X chooses action 8, the probability of winning is 0.5629.\n",
      "If Player X chooses action 9, the probability of winning is 0.6471.\n"
     ]
    }
   ],
   "source": [
    "p_wins_step0=history[0][1]\n",
    "for key, value in p_wins_step0.items():\n",
    "    print(f\"If Player X chooses action {key}, \\\n",
    "    the probability of winning is {value:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b741f3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If Player X chooses action 1, the probability of winning is 0.8206.\n",
      "If Player X chooses action 2, the probability of winning is 0.6159.\n",
      "If Player X chooses action 3, the probability of winning is 0.7762.\n",
      "If Player X chooses action 4, the probability of winning is 0.7598.\n",
      "If Player X chooses action 6, the probability of winning is 0.7974.\n",
      "If Player X chooses action 7, the probability of winning is 0.7929.\n",
      "If Player X chooses action 9, the probability of winning is 0.7860.\n"
     ]
    }
   ],
   "source": [
    "p_wins_step1=history[1][1]\n",
    "for key, value in p_wins_step1.items():\n",
    "    print(f\"If Player X chooses action {key},\\\n",
    "    the probability of winning is {value:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c51cbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the game history on your computer\n",
    "with open('files/ch12/ttt_game_history.p','wb') as fp:\n",
    "    pickle.dump(history,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a0fe6",
   "metadata": {},
   "source": [
    "## 12.4.2. Animate the Whole Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4deaf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from PIL import Image\n",
    "\n",
    "frames=[]\n",
    "for i in range(6):\n",
    "    im=Image.open(f\"files/ch12/ttt_step{i}.ps\")\n",
    "    frame=np.asarray(im)\n",
    "    frames.append(frame) \n",
    "imageio.mimsave(\"files/ch12/ttt_steps.gif\",frames,duration=1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea21ada",
   "metadata": {},
   "source": [
    "If you open the file ttt_steps.gif, you'll see the following: \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/ttt_steps.gif\"/>\n",
    "\n",
    "The animation shows the game board at each stage of the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72e9512",
   "metadata": {},
   "source": [
    "## 12.4.3. Animate the Decision Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58a42e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch12util import gen_images\n",
    "\n",
    "gen_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a728554",
   "metadata": {},
   "source": [
    "The above script highlights the decision making process of Player X. For example, if you open the file ttt_stage4step3.png, you'll see the following picture.\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/ttt_stage4step3.png\" /> It shows the probabilities of Player X winning the game with each hypothetical move. In particular, the probability is 100% if Player X chooses Cell 9. The cell is highlighted in blue, and that is also the move made by Player X as a result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd048656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "frames=[]\n",
    "for stage in [0, 2, 4]:\n",
    "    for step in [1,2,3]:\n",
    "        file=f\"files/ch12/ttt_stage{stage*2}step{step}.png\"\n",
    "        im=Image.open(file)\n",
    "        f1=np.asarray(im)\n",
    "        frames.append(f1)  \n",
    "imageio.mimsave('files/ch12/ttt_DL_probs.gif',frames,duration=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada070e0",
   "metadata": {},
   "source": [
    "If you open the file ttt_DL_probs.gif, you'll see the animation as follows.\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/ttt_DL_probs.gif\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7dbb5c",
   "metadata": {},
   "source": [
    "## 12.4.4. Animate Board Positions and the Decision Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a93a1393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch12util import combine_animation\n",
    "\n",
    "frames=combine_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd34819",
   "metadata": {},
   "source": [
    "If you open the gif file, you'll see the following animation:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/ttt_DL_steps.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4ffacb",
   "metadata": {},
   "source": [
    "## 12.4.5. Subplots of the Decision-Making Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3c5ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_frames=frames[2::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "049cec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,30),dpi=200)\n",
    "for i in range(3):\n",
    "    plt.subplot(3,1,i+1)\n",
    "    plt.imshow(subplot_frames[i])\n",
    "    plt.axis('off')\n",
    "plt.subplots_adjust(bottom=0.001,right=0.999,top=0.999,\n",
    "        left=0.001, hspace=-0.01,wspace=-0.22)\n",
    "plt.savefig(\"files/ch12/subplots_ttt.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcbe496",
   "metadata": {},
   "source": [
    "# 12.6 Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccf3b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer to question 12.2\n",
    "import time\n",
    "import random\n",
    "\n",
    "env=ttt()\n",
    "state=env.reset()   \n",
    "env.render()\n",
    "# Play a full game\n",
    "while True:\n",
    "    action = random.choice(env.validinputs)\n",
    "    time.sleep(1)\n",
    "    print(f\"Player X has chosen action {action}\")    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)[::-1]}\")    \n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"Player X has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break   \n",
    "    action = int(input(\"Player O, enter your move:\"))\n",
    "    time.sleep(1)\n",
    "    print(f\"Player O has chosen action {action}\")    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)[::-1]}\") \n",
    "    if done:\n",
    "        print(f\"Player O has won!\") \n",
    "        break   \n",
    "env.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3bdc521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer to question 12.3\n",
    "env=ttt()\n",
    "state=env.reset()   \n",
    "env.render()\n",
    "# Play a full game manually\n",
    "while True:\n",
    "    action=random.choice(env.validinputs)\n",
    "    print(f\"Player X has chosen action {action}\")    \n",
    "    state,reward,done,info=env.step(action)\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)[::-1]}\")\n",
    "    env.render()\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"Player X has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break    \n",
    "    # Use the best_move_O() function to select move\n",
    "    action=best_move_O(env)\n",
    "    print(f\"Player O has chosen action {action}\")    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)[::-1]}\")\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(f\"Player O has won!\") \n",
    "        break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48a4c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer to question 12.4\n",
    "p_wins_step2=history[2][1]\n",
    "for key, value in p_wins_step2.items():\n",
    "    print(f\"If Player X chooses action {key},\\\n",
    "    the probability of winning is {value:.4f}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
